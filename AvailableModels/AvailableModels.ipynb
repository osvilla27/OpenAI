{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51597b64-d5a1-4bbe-9384-d001c0dcda43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai (from -r requirements.txt (line 1))\n",
      "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\projects\\openai\\availablemodels\\venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (2.31.0)\n",
      "Collecting tqdm (from openai->-r requirements.txt (line 1))\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting aiohttp (from openai->-r requirements.txt (line 1))\n",
      "  Using cached aiohttp-3.8.4-cp310-cp310-win_amd64.whl (319 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\projects\\openai\\availablemodels\\venv\\lib\\site-packages (from requests>=2.20->openai->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\projects\\openai\\availablemodels\\venv\\lib\\site-packages (from requests>=2.20->openai->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\projects\\openai\\availablemodels\\venv\\lib\\site-packages (from requests>=2.20->openai->-r requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\projects\\openai\\availablemodels\\venv\\lib\\site-packages (from requests>=2.20->openai->-r requirements.txt (line 1)) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\projects\\openai\\availablemodels\\venv\\lib\\site-packages (from aiohttp->openai->-r requirements.txt (line 1)) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->openai->-r requirements.txt (line 1))\n",
      "  Using cached multidict-6.0.4-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai->-r requirements.txt (line 1))\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->openai->-r requirements.txt (line 1))\n",
      "  Using cached yarl-1.9.2-cp310-cp310-win_amd64.whl (61 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->openai->-r requirements.txt (line 1))\n",
      "  Using cached frozenlist-1.3.3-cp310-cp310-win_amd64.whl (33 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->openai->-r requirements.txt (line 1))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\projects\\openai\\availablemodels\\venv\\lib\\site-packages (from tqdm->openai->-r requirements.txt (line 1)) (0.4.6)\n",
      "Installing collected packages: tqdm, multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.8 tqdm-4.65.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe6328-2d19-423f-86db-9ac2fe7fbd2d",
   "metadata": {},
   "source": [
    "# **Available Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2afe7-3e2a-4b72-ae00-e5d114035e15",
   "metadata": {},
   "source": [
    "## **GPT-3: Processing and Generating Natural Language**\n",
    "\n",
    "The GPT-3 model is capable of understanding human language and text that appears to be natural\n",
    "language. This model family comes in a series of 4 models (A, B, C, D) that are more or less fast and\n",
    "performant.\n",
    "\n",
    "- D: **text-davinci-003**. This is the most capable GPT-3 model as it can perform what all the other models can do. In addition,\n",
    "it offers a higher quality compared to the others. It is the most recent model, as it was trained with\n",
    "data dating up to June 2021.\n",
    "\n",
    "- C: **text-curie-001**. The text-curie-001 model is the second most capable GPT-3 model as it supports up to 2048 tokens.\n",
    "Its advantage is that it is more cost-efficient than text-davinci-003 but still has high accuracy.\n",
    "It was trained with data dating up to October 2019, so it is slightly less accurate than text-davinci-003.\n",
    "It could be a good option for translation, complex classification, text analysis, and summaries.\n",
    "\n",
    "- B: **text-babbage-001**. Same as Curie: 2,048 tokens and data training up to October 2019.\n",
    "This model is effective for simpler categorizations and semantic classification.\n",
    "\n",
    "- A: **text-ada-001**. Same as Curie: 2,048 tokens and data training up to October 2019.\n",
    "This model is very fast and cost-effective, to be preferred for the simplest classifications, text\n",
    "extraction, and address correction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b1bbd-fa11-4bc8-8f39-91837f11a39a",
   "metadata": {},
   "source": [
    "## **Codex: Understanding and Generating Computer Code**\n",
    "\n",
    "OpenAI proposes two Codex models for understanding and generating computer code: code-davinci-\n",
    "002 and code-cushman-001.\n",
    "Codex is the model that powers GitHub Copilot. It is proficient in more than a dozen programming\n",
    "languages including Python, JavaScript, Go, Perl, PHP, Ruby, Swift, TypeScript, SQL and, Shell.\n",
    "Codex is capable of understanding basic instructions expressed in natural language and carrying\n",
    "out the requested tasks on behalf of the user.\n",
    "Two models are available for Codex:\n",
    "\n",
    "- **code-davinci-002**. The Codex model is the most capable. It excels at translating natural language into code. Not only\n",
    "does it complete the code, but it also supports the insertion of supplementary elements. It can handle\n",
    "up to 8,000 tokens and was trained with data dating up to June 2021.\n",
    "\n",
    "- **code-cushman-001**. Cushman is powerful and fast. Even if Davinci is more powerful when it comes to analyzing complex\n",
    "tasks, this model has the capability for many code generation tasks.\n",
    "It is also faster, and more affordable than Davinci."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915fcbeb-ecf1-4b9f-bcb9-4be77eda06bf",
   "metadata": {},
   "source": [
    "## **Content Filter**\n",
    "As its name suggests, this is a filter for sensitive content.\n",
    "Using this filter you can detect API-generated text that could be sensitive or unsafe. This filter can\n",
    "classify text into 3 categories:\n",
    "\n",
    "- safe,\n",
    "- sensitive,\n",
    "- unsafe.\n",
    "  \n",
    "If you are building an application that will be used by your users, you can use the filter to detect if\n",
    "the model is returning any inappropriate content.content.ci.o June 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a38b0-f5a6-4694-9740-ee02b12d7f42",
   "metadata": {},
   "source": [
    "## **Listing all Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5532dbd8-e53b-4c5f-9c2d-0cbae71e6215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whisper-1\n",
      "babbage\n",
      "davinci\n",
      "text-davinci-edit-001\n",
      "babbage-code-search-code\n",
      "text-similarity-babbage-001\n",
      "code-davinci-edit-001\n",
      "text-davinci-001\n",
      "ada\n",
      "babbage-code-search-text\n",
      "babbage-similarity\n",
      "code-search-babbage-text-001\n",
      "text-curie-001\n",
      "code-search-babbage-code-001\n",
      "text-ada-001\n",
      "text-similarity-ada-001\n",
      "curie-instruct-beta\n",
      "ada-code-search-code\n",
      "ada-similarity\n",
      "code-search-ada-text-001\n",
      "text-search-ada-query-001\n",
      "davinci-search-document\n",
      "ada-code-search-text\n",
      "text-search-ada-doc-001\n",
      "davinci-instruct-beta\n",
      "text-similarity-curie-001\n",
      "code-search-ada-code-001\n",
      "ada-search-query\n",
      "text-search-davinci-query-001\n",
      "curie-search-query\n",
      "davinci-search-query\n",
      "babbage-search-document\n",
      "ada-search-document\n",
      "text-search-curie-query-001\n",
      "text-search-babbage-doc-001\n",
      "curie-search-document\n",
      "text-search-curie-doc-001\n",
      "babbage-search-query\n",
      "text-babbage-001\n",
      "text-search-davinci-doc-001\n",
      "text-search-babbage-query-001\n",
      "curie-similarity\n",
      "text-embedding-ada-002\n",
      "gpt-3.5-turbo-0613\n",
      "curie\n",
      "gpt-3.5-turbo-16k-0613\n",
      "text-similarity-davinci-001\n",
      "text-davinci-002\n",
      "gpt-3.5-turbo-16k\n",
      "text-davinci-003\n",
      "davinci-similarity\n",
      "gpt-3.5-turbo-0301\n",
      "gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "def init_api():\n",
    "    with open(\".env\") as env:\n",
    "        for line in env:\n",
    "            key, value = line.strip().split(\"=\")\n",
    "            os.environ[key] = value\n",
    "        \n",
    "    openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    openai.organization = os.environ.get(\"ORG_ID\")\n",
    "\n",
    "init_api()\n",
    "models = openai.Model.list()\n",
    "for model in models[\"data\"]:\n",
    "    print(model[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18112a2-23e0-4a36-b53d-d4ccab4181d7",
   "metadata": {},
   "source": [
    "# **Using GPT Text Completions**\n",
    "\n",
    "Once you have authenticated your application, you can start using the OpenAI API to perform\r\n",
    "completions. To do this, you need to use the OpenAI Completion API.\r\n",
    "The OpenAI Completion API enables developers to access OpenAI’s datasets and models, making\r\n",
    "completions effortless.\r\n",
    "Begin by providing the start of a sentence. The model will then predict one or more possible\r\n",
    "completions, each with an associated score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91d66663-8cbf-4d23-9739-0a21dd30d522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-7UPaG2OlEHiOSyKO2U2yuWOUuQSbp\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687482540,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \" hoy?\\n\\nHoy\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 9,\n",
      "    \"completion_tokens\": 7,\n",
      "    \"total_tokens\": 16\n",
      "  }\n",
      "}\n",
      " hoy?\n",
      "\n",
      "Hoy\n"
     ]
    }
   ],
   "source": [
    "next = openai.Completion.create(model=\"text-davinci-003\",\n",
    "                                prompt=\"Qué día de la semena es\",\n",
    "                                max_tokens=7,\n",
    "                                temperature=0\n",
    "                               )\n",
    "\n",
    "print(next)\n",
    "print(next.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5074f7b1-a1d7-4d03-a743-10ea3fdd70b0",
   "metadata": {},
   "source": [
    "This result has an index of 0. The API also returned the “finish_reason”, which was “length” in this case.\n",
    "The length of the output is determined by the API, based on the “max_tokens” value provided by\n",
    "the user. In our case, we set this value to 7.\n",
    "\n",
    "Note: Tokens, by definition, are common sequences of characters in the output text. A good way\n",
    "to remember is that one token usually means about 4 letters of text for normal English words. This\n",
    "means that 100 tokens are about the same as 75 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61da6ae-a7fd-4b19-8318-5b3547ba4998",
   "metadata": {},
   "source": [
    "### **Controlling the Output’s Token Count**\n",
    "\n",
    "Let’s test with a longer example, which means a greater number of tokens (15):\n",
    "Once upon a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79f671c7-2c60-48fe-974d-230f33fc9e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-7UPZ1iR7KSxmXyx2oPR6mUY49Ho3v\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687482463,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \" una princesa que viv\\u00eda en un castillo. Era m\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 6,\n",
      "    \"completion_tokens\": 15,\n",
      "    \"total_tokens\": 21\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "next = openai.Completion.create(model=\"text-davinci-003\",\n",
    "                                prompt=\"Érase una vez\",\n",
    "                                max_tokens=15,\n",
    "                                temperature=0\n",
    "                               )\n",
    "\n",
    "print(next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669bd893-6f9a-42e3-8c1b-530b33c1bb14",
   "metadata": {},
   "source": [
    "### **Logprobs**\n",
    "\n",
    "To increase the possibilities, we can use the “logprobs” parameter. For example, setting logprobs to\r\n",
    "2 will return two versions of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f694cec-3836-4b0b-88c8-ae3dcc49f0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-7UPk14Seq5s1RrYTt9iuv8bNAN2SZ\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687483145,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \" una princesa que viv\\u00eda en un castillo. Era m\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"tokens\": [\n",
      "          \" un\",\n",
      "          \"a\",\n",
      "          \" princes\",\n",
      "          \"a\",\n",
      "          \" que\",\n",
      "          \" v\",\n",
      "          \"iv\",\n",
      "          \"\\u00eda\",\n",
      "          \" en\",\n",
      "          \" un\",\n",
      "          \" cast\",\n",
      "          \"illo\",\n",
      "          \".\",\n",
      "          \" Era\",\n",
      "          \" m\"\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          -0.17570852,\n",
      "          -0.5495857,\n",
      "          -1.0853332,\n",
      "          -0.001761513,\n",
      "          -0.8632536,\n",
      "          -0.10063593,\n",
      "          -0.00031691935,\n",
      "          -0.0046087205,\n",
      "          -0.08829467,\n",
      "          -0.023945194,\n",
      "          -1.1286361,\n",
      "          -7.584048e-06,\n",
      "          -1.3497478,\n",
      "          -1.234629,\n",
      "          -0.538533\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          {\n",
      "            \" un\": -0.17570852,\n",
      "            \",\": -2.649742\n",
      "          },\n",
      "          {\n",
      "            \"a\": -0.5495857,\n",
      "            \" re\": -2.3296149\n",
      "          },\n",
      "          {\n",
      "            \" princes\": -1.0853332,\n",
      "            \" ni\": -2.0295749\n",
      "          },\n",
      "          {\n",
      "            \"a\": -0.001761513,\n",
      "            \"ita\": -6.35068\n",
      "          },\n",
      "          {\n",
      "            \" que\": -0.8632536,\n",
      "            \" ll\": -1.0486726\n",
      "          },\n",
      "          {\n",
      "            \" v\": -0.10063593,\n",
      "            \" era\": -3.5735765\n",
      "          },\n",
      "          {\n",
      "            \"iv\": -0.00031691935,\n",
      "            \"ivia\": -8.18285\n",
      "          },\n",
      "          {\n",
      "            \"\\u00eda\": -0.0046087205,\n",
      "            \"i\": -5.419475\n",
      "          },\n",
      "          {\n",
      "            \" en\": -0.08829467,\n",
      "            \" con\": -3.1361642\n",
      "          },\n",
      "          {\n",
      "            \" un\": -0.023945194,\n",
      "            \" el\": -4.123564\n",
      "          },\n",
      "          {\n",
      "            \" cast\": -1.1286361,\n",
      "            \"a\": -1.6467179\n",
      "          },\n",
      "          {\n",
      "            \"illo\": -7.584048e-06,\n",
      "            \"ilo\": -12.849008\n",
      "          },\n",
      "          {\n",
      "            \".\": -1.3497478,\n",
      "            \" m\": -2.0818713\n",
      "          },\n",
      "          {\n",
      "            \" Era\": -1.234629,\n",
      "            \" La\": -1.75172\n",
      "          },\n",
      "          {\n",
      "            \" m\": -0.538533,\n",
      "            \" un\": -1.5180663\n",
      "          }\n",
      "        ],\n",
      "        \"text_offset\": [\n",
      "          13,\n",
      "          16,\n",
      "          17,\n",
      "          25,\n",
      "          26,\n",
      "          30,\n",
      "          32,\n",
      "          34,\n",
      "          36,\n",
      "          39,\n",
      "          42,\n",
      "          47,\n",
      "          51,\n",
      "          52,\n",
      "          56\n",
      "        ]\n",
      "      },\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 6,\n",
      "    \"completion_tokens\": 15,\n",
      "    \"total_tokens\": 21\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "next = openai.Completion.create(model=\"text-davinci-003\",\n",
    "                                prompt=\"Érase una vez\",\n",
    "                                max_tokens=15,\n",
    "                                temperature=0,\n",
    "                                logprobs=2\n",
    "                                )\n",
    "\n",
    "print(next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97ba86b-2bbf-4db5-a9af-f32af6984ab8",
   "metadata": {},
   "source": [
    "You can see that each token has a probability or score associated with it. The API will return “there”\n",
    "between “\\n” and “there” since -1.1709108 is less than -0.9263134 (top_logprobs).\n",
    "The API will select “was” instead of “lived” since -0.2422086 is greater than -2.040701. Similarly, this\n",
    "will be the case for other values.\n",
    "\n",
    "\n",
    "Each token has two possible values. The API returns the probability of each one and the sentence\r\n",
    "formed by the tokens with the highest probability We can increase the size to 5. According to OpenAI, the maximum value for logprobs is 5.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32069a2f-0652-4959-b138-8dfed90a24ba",
   "metadata": {},
   "source": [
    "### **Controlling Creativity: The Sampling Temperature**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1add7-c12a-401a-829a-956eadbbf999",
   "metadata": {},
   "source": [
    "The next parameter we can customize is the temperature. This can be used to make the model more\n",
    "creative, but creativity comes with some risks.\n",
    "For a more creative application, we could use higher temperatures such as 0.2, 0.3, 0.4, 0.5, and 0.6.\n",
    "The maximum temperature is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e295da90-450b-4230-b38f-ddc6eb334575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-7UPqeTe8ctd5t98VX4dEeOZrdLLd6\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687483556,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \" yo gritando Via Prin Mer burbufor min cocatsulfit hamb\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 6,\n",
      "    \"completion_tokens\": 15,\n",
      "    \"total_tokens\": 21\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "next = openai.Completion.create(model=\"text-davinci-003\",\n",
    "                                prompt=\"Érase una vez\",\n",
    "                                max_tokens=15,\n",
    "                                temperature=2\n",
    "                                )\n",
    "\n",
    "print(next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86196212-d24e-403f-98de-16ccfaa059e1",
   "metadata": {},
   "source": [
    "### **Sampling with “top_p”**\n",
    "Alternatively, we could use the top_p parameter. For example, using 0.5 means only the tokens with\n",
    "the highest probability mass, comprising 50%, are considered. Using 0.1 means the tokens with the\n",
    "highest probability mass, comprising 10%, are considered.\n",
    "\n",
    "It is recommended to either use the top_p parameter or the temperature parameter but not both.\r\n",
    "The top_p parameter is also called nucleus sampling or top-p sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "889e676b-4093-4099-b9b4-d4fa7d837a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-7UPuxkGvR0uwPep2IFogNKCYeNRNC\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687483823,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \", hace mucho tiempo, un reino hermoso\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 6,\n",
      "    \"completion_tokens\": 15,\n",
      "    \"total_tokens\": 21\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "next = openai.Completion.create(model=\"text-davinci-003\",\n",
    "                                prompt=\"Érase una vez\",\n",
    "                                max_tokens=15,\n",
    "                                top_p=.9,\n",
    "                                )\n",
    "\n",
    "print(next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829cc08-6d56-4ff8-b8f3-0d42ec58a17a",
   "metadata": {},
   "source": [
    "### **Streaming the Results**\n",
    "\n",
    "Another common parameter we can use in OpenAI is the stream. It’s possible to instruct the API to\n",
    "return a stream of tokens instead of a block containing all tokens. In this case, the API will return a\n",
    "generator that yields tokens in the order they were generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "392b40d1-972a-4165-afd8-7f4fea5bb66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "{\n",
      "  \"id\": \"cmpl-7UQ2xnH3no6jx8ZWuDhAPUtU0oVCB\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687484319,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \" un\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"text-davinci-003\"\n",
      "}\n",
      "{\n",
      "  \"id\": \"cmpl-7UQ2xnH3no6jx8ZWuDhAPUtU0oVCB\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687484319,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"a\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"text-davinci-003\"\n",
      "}\n",
      "{\n",
      "  \"id\": \"cmpl-7UQ2xnH3no6jx8ZWuDhAPUtU0oVCB\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687484319,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \" princes\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"text-davinci-003\"\n",
      "}\n",
      "{\n",
      "  \"id\": \"cmpl-7UQ2xnH3no6jx8ZWuDhAPUtU0oVCB\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687484319,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"a\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"text-davinci-003\"\n",
      "}\n",
      "{\n",
      "  \"id\": \"cmpl-7UQ2xnH3no6jx8ZWuDhAPUtU0oVCB\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687484319,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \" que\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"text-davinci-003\"\n",
      "}\n",
      "{\n",
      "  \"id\": \"cmpl-7UQ2xnH3no6jx8ZWuDhAPUtU0oVCB\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687484319,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \" v\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"text-davinci-003\"\n",
      "}\n",
      "{\n",
      "  \"id\": \"cmpl-7UQ2xnH3no6jx8ZWuDhAPUtU0oVCB\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687484319,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"iv\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"text-davinci-003\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "next = openai.Completion.create(model=\"text-davinci-003\",\n",
    "                                prompt=\"Érase una vez\",\n",
    "                                max_tokens=7,\n",
    "                                stream=True,\n",
    "                                )\n",
    "\n",
    "print(type(next))\n",
    "\n",
    "print(*next, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39417e23-5b90-49ab-907e-42ba0b4091e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in next:\n",
    "    print(i['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83959c2-1e4a-4de6-8ea4-db2369edec92",
   "metadata": {},
   "source": [
    "------------------------- Revisar ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df007e64-6d85-4a41-acc3-81ffde9c6a4c",
   "metadata": {},
   "source": [
    "## **Examples:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a47cc4-f374-47f3-afc9-39fe453c8f4e",
   "metadata": {},
   "source": [
    "### **Extracting keywords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9736fb0d-26af-4bed-87fa-42af18767639",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Python es un lenguaje de programación ampliamente utilizado en las aplicaciones web, el\\\n",
    "desarrollo de software, la ciencia de datos y el machine learning (ML). Los desarrolladores utilizan\\\n",
    "Python porque es eficiente y fácil de aprender, además de que se puede ejecutar en muchas plataformas\\\n",
    "diferentes. El software Python se puede descargar gratis, se integra bien a todos los tipos de sistemas\\\n",
    "y aumenta la velocidad del desarrollo.\"\n",
    "prompt = prompt + \"\\n\\nPalabras clave:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0dc53f9f-4761-42d7-90ab-e29235827dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-7UQHQMuKb1tZgQjF8V19iujca9whB\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687485216,\n",
      "  \"model\": \"text-davinci-002\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \" Python, lenguaje de programaci\\u00f3n, software\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 148,\n",
      "    \"completion_tokens\": 12,\n",
      "    \"total_tokens\": 160\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tweet = openai.Completion.create(model=\"text-davinci-002\",\n",
    "                                 prompt=prompt,\n",
    "                                 temperature=0.5,\n",
    "                                 max_tokens=300\n",
    "                                )\n",
    "\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd6199-f1f4-40fe-a1ae-0a32a9167f33",
   "metadata": {},
   "source": [
    "### **Generating Tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f85e76c-b70e-4ab3-99ee-513e84008355",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Python es un lenguaje de programación ampliamente utilizado en las aplicaciones web, el\\\n",
    "desarrollo de software, la ciencia de datos y el machine learning (ML). Los desarrolladores utilizan\\\n",
    "Python porque es eficiente y fácil de aprender, además de que se puede ejecutar en muchas plataformas\\\n",
    "diferentes. El software Python se puede descargar gratis, se integra bien a todos los tipos de sistemas\\\n",
    "y aumenta la velocidad del desarrollo.\"\n",
    "prompt = prompt + \"\\n\\nTweet en español:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b73a9be-704f-4e25-8631-3caee24f7090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-7UQKwcwDAHBccCdfpArYAn6eFrm3T\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1687485434,\n",
      "  \"model\": \"text-davinci-002\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\nPython es un lenguaje de programaci\\u00f3n muy utilizado en aplicaciones web, software, ciencia de datos y machine learning. Los desarrolladores lo usan porque es eficiente y f\\u00e1cil de aprender, adem\\u00e1s de que funciona en muchas plataformas diferentes. El software Python se puede descargar gratis, se integra bien a todos los tipos de sistemas y acelera el desarrollo.\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 149,\n",
      "    \"completion_tokens\": 120,\n",
      "    \"total_tokens\": 269\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tweet = openai.Completion.create(model=\"text-davinci-002\",\n",
    "                                 prompt=prompt,\n",
    "                                 temperature=0.5,\n",
    "                                 max_tokens=300\n",
    "                                )\n",
    "\n",
    "print(tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
